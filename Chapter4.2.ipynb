{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ef2edd",
   "metadata": {},
   "source": [
    "## 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce65c1",
   "metadata": {},
   "source": [
    "- 경사법에서는 기울기(경사) 값을 시준으로 나아갈 방향을 정함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18fd497",
   "metadata": {},
   "source": [
    "### 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfdb08",
   "metadata": {},
   "source": [
    "- 미분 : 특정 순간의 변화량\n",
    "\n",
    "- 좌변은 f(x)의 x에 대한 미분(x에 대한 f(x)의 변화량)을 나타내는 기호\n",
    "- 결국, x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키냐를 의미\n",
    "- 이때 시간의 작은 변화, 즉 시간을 뜻하는 h를 한없이 0에 가깝게 한다는 의미를 lim h -> 0으로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e66422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 계산 파이썬 구현\n",
    "# 나쁜 구현 예\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50 \n",
    "    return (f(x + h) - f(x)) / (h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f36097",
   "metadata": {},
   "source": [
    "- 함수의 이름은 수치 미분(numerical differentiation)에서 따옴\n",
    "\n",
    "- 이 함수는 '함수 f'와 '함수 f에 넘길 인수 x'라는 두 인수를 받음\n",
    "- 실제로는 개선해야 할 점이 2개 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3980fad",
   "metadata": {},
   "source": [
    "- 앞의 구현에서는 h에 가급적 작은 값을 대입하고 싶었기네 10e - 50이라는 작은 값을 이용\n",
    "- 이 값은 0.00...1 형태에서 소수점 아래 0이 49개라는 의미\n",
    "\n",
    "- 그러나 이 방식은 반올림 오차(rounding error) 문제를 일으킴\n",
    "- 반올림 오차는 작은 값이 생략되어 최종 계산 결과에 오차가 생기게 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ed28f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 반올림 오차 예\n",
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c6c2f",
   "metadata": {},
   "source": [
    "- 첫 번째 개선 포인트\n",
    "\n",
    "- 이와 같이 1e-50을 float32형(32비트 부동소수점)으로 나타내면 0.0이 되어, 올바르게 표현 불가\n",
    "- 너무 작은 값을 이용하면 컴퓨터로 계산하는 데 문제가 됨!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39a66a",
   "metadata": {},
   "source": [
    "- 두 번째 개선\n",
    "\n",
    "- 함수 f의 차분 관련\n",
    "- 앞의 구현에서는 x + h와 x 사이의 함수 f의 차분을 계산하고 있지만, 이 계산에는 오차가 있음\n",
    "- 진정한 미분은 x 위치의 함수의 기울기(접선)에 해당하지만, 이번 구현에서 미분은 (x + h)와 x 사이의 기울기에 해당\n",
    "- 이 차이는 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c0e6a",
   "metadata": {},
   "source": [
    "- 오차를 줄이기 위해 (x + h)와 (x - h)일 때의 함수 f의 차분을 계산하는 방법을 쓰기도 함\n",
    "\n",
    "- 이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 '중심 차분' 혹은 '중앙 차분'이라 함\n",
    "- (x + h)와 x의 차분은 '전방 차분'이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21feab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선점 적용 수치 미분 구현\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1906f3",
   "metadata": {},
   "source": [
    "- 아주 작은 차분으로 미분하는 것을 '수치 미분'이라 함\n",
    "\n",
    "- 한편, 수식을 전개해 미분하는 것을 해석적(analytic)이라는 말을 이용하여 '해석적 헤' 혹은 '해석적으로 미분하다' 등으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae69f01",
   "metadata": {},
   "source": [
    "### 수치 미분의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61347ca6",
   "metadata": {},
   "source": [
    "- 2차 함수를 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fba86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차 함수 구현\n",
    "def function_1(x):\n",
    "    return 0.01 * x**2 + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8910f9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 함수 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ac056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = 5에서의 수치 미분\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c58d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = 10에서의 수치 미분\n",
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc1daa",
   "metadata": {},
   "source": [
    "- 이렇게 계산한 미분 값이 x에 대한 f(x)의 변화량\n",
    "- 함수의 기울기에 해당\n",
    "\n",
    "- 앞의 수치 미분과 결과를 비교하면 그 오차가 매우 작음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd668d7c",
   "metadata": {},
   "source": [
    "### 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08eb9ae",
   "metadata": {},
   "source": [
    "- 인수들의 제곱 합을 계산하는 단순한 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ff60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 또는 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21893024",
   "metadata": {},
   "source": [
    "- 인수 x는 넘파이 배열이라고 가정\n",
    "\n",
    "- 이 코드는 넘파이 배열의 각 원소를 제곱하고 그 합을 구할 뿐인 간단한 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e434eb",
   "metadata": {},
   "source": [
    "- 주의할 점\n",
    "\n",
    "- 변수가 2개!\n",
    "- '어떤 변수에 대한 미분이냐'를 구별해야 함\n",
    "- 변수가 여럿인 함수에 대한 미분을 '편미분'이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894345bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = 3, x1 = 4일 때 x0에 대한 편미분\n",
    "def function_tmp1(x0):\n",
    "    return x0 * x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79aaf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027aef90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x0 = 3, x1 = 4일 때 x1에 대한 편미분\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1 * x1\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dc8ba",
   "metadata": {},
   "source": [
    "- 이 문제들은 변수가 하나인 함수를 정의하고, 그 함수를 미분하는 형태로 구현\n",
    "\n",
    "- 해석적 미분의 결과와 거의 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80e3df",
   "metadata": {},
   "source": [
    "- 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구함\n",
    "\n",
    "- 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정함\n",
    "- 앞의 예에서는 목표 변수를 제외한 나머지를 특정 값에 고정하기 위해서 새로운 함수를 정의함\n",
    "- 그리고 그 새로 정의한 함수에 대해 그동안 사용한 수치 미분 함수를 적용하여 편미분을 구한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb07ef9",
   "metadata": {},
   "source": [
    "## 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca69f7",
   "metadata": {},
   "source": [
    "- x0과 x1의 편미분을 동시에 계산하고 싶다면?\n",
    "\n",
    "- 모든 변수의 편미분을 벡터로 정리한 것을 기울기(gradient)라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ab6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numercial_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x) # f(x + h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x - h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7c566",
   "metadata": {},
   "source": [
    "- 동작 방식은 변수가 하나일 때의 수치 미분과 거의 같음\n",
    "\n",
    "- np.zeros_like(x)는 x와 형상이 같고 그 원소가 모두 0인 배열을 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ed07f",
   "metadata": {},
   "source": [
    "- nimerical_gradient(f, x) 함수의 인수인 f는 함수이고 x는 넘파이 베열이므로 넘파이 배열 x의 각 원소에 대해서 수치 미분을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e241d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# (3, 4), (0, 2), (3, 0)에서의 기울기\n",
    "print(numercial_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numercial_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numercial_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118eaaa3",
   "metadata": {},
   "source": [
    "- 이처럼 (x0, x1)의 각 점에서의 기울기를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5737",
   "metadata": {},
   "source": [
    "- 기울기 그림은 방향을 가진 벡터(화살표)로 그려짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536df0a",
   "metadata": {},
   "source": [
    "- 기울기는 각 지점에서 낮아지는 방향을 가리킴\n",
    "\n",
    "- 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1edb888",
   "metadata": {},
   "source": [
    "### 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a30a8",
   "metadata": {},
   "source": [
    "- 기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아냄\n",
    "- 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 함\n",
    "\n",
    "- 여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값\n",
    "- 그러나 일반적인 문제의 손실 함수는 매우 복잡\n",
    "- 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없음\n",
    "- 이런 상황에서 기울기를 잘 이용해 함수의 최솟값(또는 가능한 작은 값)을 찾으려는 겻이 경사법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389613f",
   "metadata": {},
   "source": [
    "- 주의할 점\n",
    "\n",
    "- 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표는 기울기\n",
    "- 그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아갈 방향인지는 보장할 수 없음\n",
    "- 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c3852",
   "metadata": {},
   "source": [
    "- 함수가 극솟값, 최솟값, 또 안장점(saddle point)이 되는 장소에서는 기울이가 0\n",
    "\n",
    "- 극솟값은 국소적인 최솟값, 즉 한정된 범위에서의 최솟값인 점\n",
    "- 어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점\n",
    "- 경사법은 기울기가 0인 장소를 찾지만 그것이 반드시 최솟값이라고는 할 수 없음(극솟값이나 안장점일 가능성이 있음)\n",
    "- 복잡하고 찌그러진 모양의 함수라면 (대부분) 평평한 곳으로 파고들면서 고원(plateau)라 하는 학습이 진행되지 않는 정체기에 빠질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd936a32",
   "metadata": {},
   "source": [
    "- 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있음\n",
    "\n",
    "- 그래서 최솟값이 되는 장소를 찾는 문제(아니면 가능한 한 작은 값이 되는 장소를 찾는 문제)에서는 기울기 정보를 단서로 나아갈 방향을 정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1633ca",
   "metadata": {},
   "source": [
    "- 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동\n",
    "\n",
    "- 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아감을 반복\n",
    "- 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법(gradient method)\n",
    "- 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법\n",
    "- 신경망 학습에서는 경사법을 많이 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55d95c",
   "metadata": {},
   "source": [
    "- 경사법은 최솟값을 찾느냐, 최댓값을 찾느냐에 따라 이름이 다름\n",
    "\n",
    "- 전자를 경사 하강법(gradient descent method), 후자를 경사 상승법(gradient ascant method)라고 함\n",
    "- 다만 손실 함수의 부호를 반전시키면 최솟값을 찾는 문제와 최댓값을 찾는 문제는 같은 것이니 하강이냐 상승이냐는 본질적으로는 중요하지 않음\n",
    "- 일반적으로 신경망(딥러닝) 분야에서의 경사법은 '경사 하강법'으로 등장할 때가 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfc10b",
   "metadata": {},
   "source": [
    "- 학습률(learning rate)\n",
    "\n",
    "- 한 번의 학습으로 얼마만큼 학습해야 할지\n",
    "- 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35504c3",
   "metadata": {},
   "source": [
    "- 변수의 값을 갱신하는 단계를 여러 번 반복하면서 서서히 함수의 값을 줄이는 것\n",
    "\n",
    "- 변수의 수가 늘어도 같은 식(각 변수의 편미분 값)으로 갱신하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d3ed5",
   "metadata": {},
   "source": [
    "- 학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해두어야 함\n",
    "\n",
    "- 일반적으로 이 값이 너무 크거나 작으면 '좋은 장소'를 찾아갈 수 없음\n",
    "- 신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db35746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 구현\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numercial_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb00fa",
   "metadata": {},
   "source": [
    "- 인수 f는 최적화하려는 함수, init_x는 초깃값, lr은 learning rate를 의미하는 학습률, step_num은 경사법에 따른 반복 횟수\n",
    "\n",
    "- 함수의 기울기는 numerical_gradient(f, x)로 구하고, 그 기울기에 학습률을 곱한 값으로 갱신하는 처리를 step_num번 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4df555",
   "metadata": {},
   "source": [
    "- 이 함수를 사용하면 함수의 극솟값을 수할 수 있고 잘하면 최솟값을 구할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deea446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사법으로 f(x0, x1)의 최솟값 구하기\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)\n",
    "print(result) # [-6.11110793e-10 8.14814391e-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c53ee0",
   "metadata": {},
   "source": [
    "- 여기에서는 초깃값을 (-3.0, 4.0)으로 설정한 후 경사법을 사용해 최솟값 탐색 시작\n",
    "\n",
    "- 최종 결과는 [-6.11110793e-10 8.14814391e-10]으로, 거의 (0, 0)에 가까운 값\n",
    "- 실제로 진정한 최솟값은 (0, 0)이므로 경사법으로 거의 정확한 결과를 얻은 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0cd82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)\n",
    "print(result) # [-2.58983747e+13 -1.29583874e+12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9215e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 작은 경우\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)\n",
    "print(result) # [-2.99999994 3.99999992]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335a4d1",
   "metadata": {},
   "source": [
    "- 이 실험 결과와 같이 학습률이 너무 크면 큰 값으로 발산해버림\n",
    "\n",
    "- 반대로 너무 작으면 거의 갱신되지 않은 채 끝나버림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a1f80",
   "metadata": {},
   "source": [
    "- 학습률 같은 매개변수를 하이퍼파라미터(hyper parameter, 초매개변수)라고 함\n",
    "\n",
    "- 이는 가중치와 편향 같은 신경망의 매개변수와는 성질이 다른 매개변수\n",
    "- 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 '자동'으로 획득되는 매개변수\n",
    "- 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수\n",
    "- 일반적으로는 이 하이퍼파라미터들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e3352",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f65083",
   "metadata": {},
   "source": [
    "- 신경망 학습 절차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8d2e7",
   "metadata": {},
   "source": [
    "- 전제\n",
    "\n",
    "- 신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 함\n",
    "\n",
    "- 신경망 학습은 4단계로 수행함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17111a8f",
   "metadata": {},
   "source": [
    "1. 1단계 - 미니배치\n",
    "\n",
    "- 훈련 데이터 중 일부를 무작위로 가져옴\n",
    "\n",
    "- 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afded3b",
   "metadata": {},
   "source": [
    "2. 2단계 - 기울기 산출\n",
    "\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함\n",
    "\n",
    "- 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77cb57c",
   "metadata": {},
   "source": [
    "3. 3단계 - 매개변수 갱신\n",
    "\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c24f04",
   "metadata": {},
   "source": [
    "4. 4단계 - 반복\n",
    "\n",
    "- 1 ~ 3단계를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7a4e6",
   "metadata": {},
   "source": [
    "- 이것이 신경망 학습이 이뤄지는 순서\n",
    "\n",
    "- 이는 경사 하강법으로 매개변수를 갱신하는 방법\n",
    "- 이때 데이터를 미니배치로 무작위로 선정하기 떄문에 확률적 경사 하강법(stochastic gradient descent, SGD)이라고 부름\n",
    "- '확률벅으로 무작위로 골라낸 데이터'에 대해 수행하는 경사 하강법이라는 의미\n",
    "- 대부분의 딥러닝 프레임워크는 확률적 경사 하강법의 영어 머리글자를 딴 SGD라는 함수로 이 기능을 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19006e",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7f306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta)) / y.shape[0]\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp = x.flat[idx]\n",
    "\n",
    "        x.flat[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x.flat[idx] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad.flat[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x.flat[idx] = tmp\n",
    "\n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)  \n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy \n",
    "\n",
    "    # x: 입력 데이터, t: 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86715097",
   "metadata": {},
   "source": [
    "- TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 가짐\n",
    "\n",
    "- params 변수에는 가중치 매개변수가 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e439aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape) # (784, 100)\n",
    "print(net.params['b1'].shape) # (100,)\n",
    "print(net.params['W2'].shape) # (100, 10)\n",
    "print(net.params['b2'].shape) # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab30b8",
   "metadata": {},
   "source": [
    "- 이와 같이 params 변수에는 이 신경망에 필요한 매개변수가 모두 저장됨\n",
    "\n",
    "- 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1da57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 처리\n",
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f56f8",
   "metadata": {},
   "source": [
    "- grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장\n",
    "\n",
    "- numerical_gradient() 메서드를 사용해 기울기를 계산하면 grads 변수에 기울기 정보가 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d8366b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10) # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 기울기 꼐산\n",
    "\n",
    "grads['W1'].shape # (784, 100)\n",
    "grads['b1'].shape # (100,)\n",
    "grads['W2'].shape # (100, 10)\n",
    "grads['b2'].shape # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0c680",
   "metadata": {},
   "source": [
    "- TwoLayerNet의 메서드\n",
    "\n",
    "- __init__(self, input_size, hidden_size, output_size) 메서드는 클래스를 초기화함(초기화 메서드는 TwoLayerNet을 생성할 때 불리는 메서드)\n",
    "\n",
    "- 인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수\n",
    "- 출력은 10개\n",
    "- 따라서 input_size = 784, output_size = 10으로 지정하고 은닉층의 개수인 hidden_size는 적당한 값을 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9110c",
   "metadata": {},
   "source": [
    "- 이 초기화 메서드에서는 가중치 매개변수도 초기화함\n",
    "\n",
    "- 가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우하기도 함\n",
    "- predict(self, x)와 accuracy(Self, x, t)의 구현은 앞에서 본 신경망의 추론 처리와 거의 같음\n",
    "- loss(self, x, t)는 손실 함수의 값을 계산하는 메서드\n",
    "- 이 메서드는 predict()의 결과와 정답 레이블을 바탕으로 교차 엔트로피 오차를 구하도록 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2655eae",
   "metadata": {},
   "source": [
    "- 남은 numercial_gradient(self, x, t) 메서드는 각 매개변수의 기울기를 계산\n",
    "\n",
    "- 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산\n",
    "- 마지막 gradient(self, x, t)는 오차역전파법을 사용하여 기울기를 효율적이고 빠르게 계산함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c00ab6",
   "metadata": {},
   "source": [
    "- numerical_gradient(self, x, t)는 수치 미분 방식으로 매개변수의 기울기를 계산\n",
    "\n",
    "- 오차역전파법을 쓰면 수치 미분을 사용할 때와 거의 같은 결과를 훨씬 빠르게 얻을 수 있음\n",
    "- 신경망 학습은 오래 걸리니, 시간을 절약하려면 gradient(self, x, t)를 쓰는 것이 좋음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5689b",
   "metadata": {},
   "source": [
    "### 미니배치 학습 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8b0bf",
   "metadata": {},
   "source": [
    "- 미니배치 학습\n",
    "\n",
    "- 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에서 경사법으로 매개변수를 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c0f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c315b",
   "metadata": {},
   "source": [
    "- 여기서는 미니배치 크기를 100으로 함\n",
    "\n",
    "- 매변 60000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려냄\n",
    "- 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신\n",
    "- 경사법에 의한 갱신 횟수(반복 횟구)를 10000번으로 설정, \n",
    "- 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고, 그 값을 배열에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f746d",
   "metadata": {},
   "source": [
    "- 학습 횟수가 늘어가면서 손실 함수의 값이 줄어든다\n",
    "\n",
    "- 이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미\n",
    "- 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가가고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8def3c",
   "metadata": {},
   "source": [
    "### 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05ee45",
   "metadata": {},
   "source": [
    "- 학습을 반복함으로써 손실 함수의 값이 서서히 내려가는 것을 확인\n",
    "\n",
    "- 이때의 손실 함수의 값이란, 정확히는 '훈련 데이터의 미니배치에 대한 손실 함수'의 값\n",
    "- 훈련 데이터의 손실 함수 값이 작아지는 것은 신경망이 잘 학습하고 있다는 방증\n",
    "- 하지만 이 결과만으로는 다른 데이터셋에도 비슷한 실력을 발휘할지는 확실하지 않음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea81e5",
   "metadata": {},
   "source": [
    "- 신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해야 함\n",
    "- - '오버피팅'을 일으키지 않는지 확인해야 함\n",
    "\n",
    "- 오버피팅되었다는 것은, 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238ef44",
   "metadata": {},
   "source": [
    "- 신경망 학습의 원래 목표는 범용적인 능력을 익히는 것\n",
    "\n",
    "- 범용 능력을 평가하려면 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해봐야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28bbffa",
   "metadata": {},
   "source": [
    "- 에폭(epoch)은 하나의 단위\n",
    "\n",
    "- 1에폭은 학습에서 훈련 데이터를 모두 소진했을 떄의 횟수에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf01c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6d5ce",
   "metadata": {},
   "source": [
    "- 이 예에서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록\n",
    "\n",
    "- 정확도를 1에폭마다 계산하는 이유는 for문 안에서 매번 계산하기에는 시간이 오래 걸리고, 그렇게 자주 기록할 필요도 없기 때문\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9633d1",
   "metadata": {},
   "source": [
    "- 에폭이 진행될수록(학습이 진행될수록) 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있음\n",
    "\n",
    "- 두 정확도에는 차이가 없음\n",
    "- 이번 학습에서는 오버피팅이 일어나지 않았음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed372025",
   "metadata": {},
   "source": [
    "## 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b71dc",
   "metadata": {},
   "source": [
    "- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용함\n",
    "\n",
    "- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가\n",
    "- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신\n",
    "- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복\n",
    "- 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라 함\n",
    "- 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있음\n",
    "- 수치 미분을 이용한 계산에는 시간이 걸리지만, 그 구현은 간단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7fa9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
