{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2738bd49",
   "metadata": {},
   "source": [
    "# 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9ab04",
   "metadata": {},
   "source": [
    "- 학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "\n",
    "- 신경망이 학습할 수 있도록 해주는 지표인 손실함수!\n",
    "- 손실 함수는 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff25329",
   "metadata": {},
   "source": [
    "## 데이터에서 학습한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa7a669",
   "metadata": {},
   "source": [
    "- 신경망의 특징은 데이터를 보고 학습할 수 있음\n",
    "\n",
    "- 데이터에서 학습한다 -> 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f688f",
   "metadata": {},
   "source": [
    "### 데이터 주도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327dce1",
   "metadata": {},
   "source": [
    "- 기계학습은 데이터가 생명!\n",
    "- - 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것\n",
    "\n",
    "- 데이터가 없으면 아무것도 시작되지 않음\n",
    "- 그래서 기계학습의 중심에는 데이터가 존재함\n",
    "- 이처럼 데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780e4b6",
   "metadata": {},
   "source": [
    "- 기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도\n",
    "\n",
    "- 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 특성을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd89635",
   "metadata": {},
   "source": [
    "- 이미지에서는 특징(feature)을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있음\n",
    "\n",
    "- 여기에서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기\n",
    "\n",
    "- 이미지의 특징은 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 많이 사용\n",
    "- 이런 특징을 사용하여 이미지 데이터를 벡터로 변환\n",
    "- 변환된 벡터를 가지고 지도 학습의 대표 분류 기법인 SVM, KNN 등으로 학습 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fec31",
   "metadata": {},
   "source": [
    "- 기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 '기계'가 담당\n",
    "\n",
    "- 무로부터 알고리즘을 설계하는 것보다 효율이 높아 문제를 해결해야 하는 사람의 부담도 덜어줌\n",
    "- 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계\n",
    "- 이 말은 문제에 적합한 특징을 쓰지 않으면(혹은 특징을 설계하지 않으면) 좋은 결과를 얻을 수 없음\n",
    "- 특징과 기계학습을 활용한 접근에도 문제에 따라서는 '사람'이 적절한 특징을 생각해내야 함!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964d2a1",
   "metadata": {},
   "source": [
    "- 신경망은 이미지를 있는 그대로 학습\n",
    "\n",
    "- 특징과 기계학습 방식에서는 특징을 사람이 설계했지만, 신경망은 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797834ba",
   "metadata": {},
   "source": [
    "- 딥러닝을 종단간 기계학습(end-to-end machine learning)이라고도 함\n",
    "\n",
    "- 종단간은 처음부터 끝까지라는 의미\n",
    "- 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f256e4",
   "metadata": {},
   "source": [
    "- 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 것\n",
    "\n",
    "- 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도\n",
    "- 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습할 수 있다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b820c79",
   "metadata": {},
   "source": [
    "### 훈련 데이터와 시험 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201691b",
   "metadata": {},
   "source": [
    "- 기계학습 문제는 데이터를 훈련 데이터(training data)와 시험 데이터(test data)로 나눠 학습과 실험을 수행\n",
    "\n",
    "- 훈련 데이터만 사용하여 학습하면서 최적의 매개변수 찾기\n",
    "- 시험 데이터를 사용하여 앞서 훈련한 모델의 실력 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b479a1",
   "metadata": {},
   "source": [
    "- 훈련 데이터와 시험 데이터 나눠야 하는 이유\n",
    "\n",
    "- -> 우리가 원하는 것은 범용적으로 사용할 수 있는 모델이기 때문\n",
    "- 이 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32267692",
   "metadata": {},
   "source": [
    "- 범용 능력은 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력\n",
    "\n",
    "- 이 범용 능력을 획득하는 것이 기계학습의 최종 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71279f",
   "metadata": {},
   "source": [
    "- 데이터셋 하나로만 매개변수의 학습과 평가를 수행하면 올바른 평가가 될 수 없다\n",
    "\n",
    "- 수중의 데이터셋은 제대로 맞히더라도 다른 데이터셋에는 엉망인 일도 벌어짐\n",
    "- 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅(overfitting)이라 함\n",
    "- 오버피팅을 피하기는 기계학습의 중요한 과제!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d45cc1",
   "metadata": {},
   "source": [
    "## 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4236f",
   "metadata": {},
   "source": [
    "- 신경망 학습에서는 현재의 상태를 하나의 지표로 표현함\n",
    "\n",
    "- 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것\n",
    "- 신경망은 하나의 지표를 기준으로 최적의 매개변수 값을 탐색함\n",
    "- 신경망 학습에서 사용하는 지표는 손실 함수(loss function)이라 함\n",
    "- 이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b484f4",
   "metadata": {},
   "source": [
    "- 손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표\n",
    "\n",
    "- 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하느냐를 나타냄\n",
    "- 손실함수에 마이너스를 곱하면 '얼마나 좋으냐'는 지표로 변함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c3d79",
   "metadata": {},
   "source": [
    "### 오차제곱합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff09555",
   "metadata": {},
   "source": [
    "오차제곱합(sum of squares for error, SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7261a",
   "metadata": {},
   "source": [
    "- y는 신경망의 출력(신경망이 추정한 값)\n",
    "\n",
    "- t는 정답 레이블\n",
    "- k는 데이터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1847a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d357cf",
   "metadata": {},
   "source": [
    "- 이 배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 0, 1, 2, ...일 때의 값\n",
    "\n",
    "- 여기에서 신경망의 출력 y는 소프트맥스 함수의 출력\n",
    "- 소프트맥스 함수의 출력은 확률로 해석할 수 있음\n",
    "- 정답 레이블인 t는 정답을 가리키는 위치의 원소는 1로, 그 외에는 0으로 표기\n",
    "- 이처럼 한 원소만 1호 하고 그 외에는 0으로 나타내는 표기법을 원-핫 인코딩이라 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9cb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 원소의 출력과 정답 레이블의 차를 제곱한 후 그 총합 구하기\n",
    "# 오차제곱합 구현\n",
    "def sum_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e0ac9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 정답은 2\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 예1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd26ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "# 예2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(sum_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c097ff9",
   "metadata": {},
   "source": [
    "- 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작음\n",
    "\n",
    "- 즉, 오차제곱합 기준으로는 첫 번째 추정 결과가 (오차가 더 작으니) 정답에 더 가까울 것으로 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5bf8f",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbdf25",
   "metadata": {},
   "source": [
    "교차 엔트로피 오차(cross entropy error, CEE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de6e64",
   "metadata": {},
   "source": [
    "- log는 밑이 e인 자연로그임\n",
    "\n",
    "- y는 신경망의 출력\n",
    "- t는 정답 레이블\n",
    "- - 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0 (원-핫 인코딩)\n",
    "- - 그래서 실질적으로 정답일 때의 추정의 자연로그를 계산하는 식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6107d6f2",
   "metadata": {},
   "source": [
    "- x가 1일 때 y는 0이 되고 x가 0에 가까워질수록 y의 값은 점점 작아짐\n",
    "\n",
    "- 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 됨\n",
    "- 반대로 정답일 떄의 출력이 작아질수록 오차는 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0663de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 엔트로피 오차 구현하기\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7  # log(0) 방지 위해 작은 값 더하기\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75625f6",
   "metadata": {},
   "source": [
    "- 여기에서 y와 t는 넘파이 배열\n",
    "\n",
    "- np.log를 계산할 때 아주 작은 값인 delta를 더함\n",
    "- - 이는 np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산을 진행할 수 없게 되기 때문\n",
    "- 아주 작은 값을 더해서 절대 0이 되지 않도록, 마이너스 무한대가 발생하지 않도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8faac62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n"
     ]
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55578639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d33f9",
   "metadata": {},
   "source": [
    "- 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 오차제곱합의 판단과 일치함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569483",
   "metadata": {},
   "source": [
    "### 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aeb3bf",
   "metadata": {},
   "source": [
    "- 기계학습 문제는 훈련 데이터를 사용해 학습함\n",
    "- 훈련 데이터에 대한 손실 함수 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아냄\n",
    "- 이렇게 하려면 모든 훈련 데이터를 대상으로 손실함수 값을 구해야 한다\n",
    "\n",
    "- 즉, 훈련 데이터가 100개 있으면 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45eb88",
   "metadata": {},
   "source": [
    "- 마지막에 N(데이터의 개수)으로 나누어 정규회 한다\n",
    "- N으로 나눔으로써 '평균 손실 함수'를 구함\n",
    "\n",
    "- 이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계 없이 언제든 통일된 지표를 얻을 수 있음\n",
    "- 훈련 데이터가 1000개든 10000개든 상관없이 평균 손실 함수를 구할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccf970",
   "metadata": {},
   "source": [
    "- 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 현실적이지 않음\n",
    "\n",
    "- 이런 경우 데이터 일부를 추려 전체의 '근사치'로 이용할 수 있음\n",
    "- 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행\n",
    "- 이 일부를 미니배치(mini-batch)라 함\n",
    "- 60000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만 사용하여 학습하는 것\n",
    "- 이러한 학습 방법을 '미니배치 학습'이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbf81bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터에서 지정한 수의 데이터를 무작위로 골라내는 코드 작성\n",
    "# MNIST 불러오기\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 부모 디렉터리 접근 가능하도록 설정\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "# 데이터 형상 출력\n",
    "print(x_train.shape)   # (60000, 784)\n",
    "print(t_train.shape)   # (60000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92258a",
   "metadata": {},
   "source": [
    "- load_mnist 함수는 MNIST 데이터셋을 읽어오는 함수\n",
    "\n",
    "- 이 함수는 훈련 데이터와 시험 데이터를 읽는다\n",
    "- 호출할 때 one_hot_label = True로 지정하여 원-핫 인코딩으로, 즉 정답 위치의 원소만 1이고 나버지가 0인 배열을 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae8049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 훈련 데이터에서 무작위로 10장만\n",
    "train_size = x_train.shape[0]  # 훈련 데이터 수\n",
    "batch_size = 10               # 미니배치 크기\n",
    "batch_mask = np.random.choice(train_size, batch_size)  # 무작위로 인덱스 선택\n",
    "x_batch = x_train[batch_mask]  # 선택된 인덱스에 해당하는 훈련 데이터\n",
    "t_batch = t_train[batch_mask]  # 선택된 인덱스에 해당하는 정답 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f339a9",
   "metadata": {},
   "source": [
    "- np.random.choice()로는 지정한 범위의 수 중에서 무작위로 원하는 개수만 꺼낼 수 있음\n",
    "\n",
    "- 가령 np.random.choice(60000, 10)은 0 이상 60000 미만의 수 중에서 무작위로 10개를 골라냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea101266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34852, 12057, 55698, 57179, 25201, 45614, 16683, 31377, 57910,\n",
       "       51751])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제로 돌려본 결과\n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a3642",
   "metadata": {},
   "source": [
    "- 이 함수가 출력한 배열을 미니배치로 뽑아 낼 데이터의 인덱스로 사용하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2684e",
   "metadata": {},
   "source": [
    "- 이제 무작위로 선택한 이 인덱스를 사용해 미니배치를 뽑아내기만 하면 됨\n",
    "\n",
    "- 손실함수도 이 미니배치로 계산!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2550eaf",
   "metadata": {},
   "source": [
    "- 손실함수는 일부 표본 데이터로 전체를 비슷하게 계측함\n",
    "\n",
    "- 전체 훈련 데이터의 대표로서 무작뒤오 선택한 작은 덩어리(미니배치)를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af2188",
   "metadata": {},
   "source": [
    "### (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d51f38",
   "metadata": {},
   "source": [
    "- 구현한 교차 엔트로피 오차(데이터를 하나씩 처리하는 구현)를 조금만 바꾸면 됨\n",
    "\n",
    "- 여기에서는 데이터가 하나인 경우와 데이터가 배치로 묶여 입력될 경우 모두를 처리할 수 있도록 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e18e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:  # 데이터가 하나인 경우\n",
    "        t = t.reshape(1, t.size)  # 정답 레이블을 2차원 배열로 변환\n",
    "        y = y.reshape(1, y.size)  # 출력도 2차원 배열로 변환\n",
    "\n",
    "    batch_size = y.shape[0]  # 배치 크기\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38b10e",
   "metadata": {},
   "source": [
    "- 이 코드에서 y는 신경망의 출력, t는 정답 레이블\n",
    "\n",
    "- y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구한느 경우는 reshape 함수로 데이터의 형상을 바꿔줌\n",
    "- 그리고 배치의 크기로 나눠 정규화하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff76f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 원-핫 인코딩이 아닐 때 교차 엔트로피 오차 구현하기\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:  # 데이터가 하나인 경우\n",
    "        t = t.reshape(1, t.size)  # 정답 레이블을 2차원 배열로 변환\n",
    "        y = y.reshape(1, y.size)  # 출력도 2차원 배열로 변환\n",
    "\n",
    "    batch_size = y.shape[0]  # 배치 크기\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307e1da",
   "metadata": {},
   "source": [
    "- 이 구현에서는 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심!\n",
    "\n",
    "- 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 잇따\n",
    "- 그래서 원-핫 인코딩 시 t * np.log(y)였던 부분을 레이블 표현일 때는 np.log(y[np.arange(batch_size), t])로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18875c70",
   "metadata": {},
   "source": [
    "- np.log(y[np.arange(batch_size), t])\n",
    "\n",
    "- np.arange(batch_size)는 0부터 batch_size - 1까지 배열을 생성함\n",
    "- batch_size가 5이면 np.arange(batch_size)는 [0, 1, 2, 3, 4]라는 넘파이 배열을 생성\n",
    "- t에는 레이블이 [2, 7, 0, 9, 4]와 같이 저장되어 있으므로 y[np.arange(batch_size), t]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1c3cb",
   "metadata": {},
   "source": [
    "### 왜 손실함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e1515",
   "metadata": {},
   "source": [
    "- 신경망 학습에서는 최적의 매개변수(가중치의 편향)을 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾음\n",
    "\n",
    "- 이떄 매개변수의 미분(정확히는 기울기)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674203d5",
   "metadata": {},
   "source": [
    "- 가상이 신경망이 있고 그 신경망의 어느 한 가중치 매개변수에 주목한다 가정\n",
    "\n",
    "- 이떄 그 가중치 매개변수의 손실 함수의 미분이란 '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐'라는 의미\n",
    "- 만약 이 미분 값이 음수면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있음\n",
    "- 반대로, 미분 값이 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다\n",
    "- 그러나 미분값이 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않음\n",
    "- - -> 가중치 매개변수의 갱신은 여기서 멈춤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22348518",
   "metadata": {},
   "source": [
    "- 정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소애서 0이 되어 매개변수를 갱신할 수 없기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672542a",
   "metadata": {},
   "source": [
    "- 신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다\n",
    "\n",
    "- 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ec3c8",
   "metadata": {},
   "source": [
    "- 정확도를 지표로 삼으면 매개변수의 미분이 대부분인 장소에서 0이 되는 이유는?\n",
    "\n",
    "- 만약 정확도가 지표였다면 매개변수를 약간만 조정해서는 정확도가 개선되지 않고 일정하게 유지됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634daf9b",
   "metadata": {},
   "source": [
    "- 손실함수를 지표로 삼는다면 매개변수의 값이 조금이라도 변하면 그에 반응하여 손실 함수의 값도 연속적으로 변화한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9b648",
   "metadata": {},
   "source": [
    "- 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다\n",
    "\n",
    "- 이는 '계단 함수'를 활성화 함수로 사용하지 않는 이유와도 들어맞음\n",
    "- 만약 활성화 함수로 계단 함수를 사용하면 지금까지 설명한 것과 같은 이유로 신경망 학습이 잘 이뤄지지 않음\n",
    "- 계단 함수의 미분은 대부분의 장소(0 이외의 곳)에서 0이다\n",
    "- 그 결과, 계단 함수를 이용하면 손실 함수를 지표로 삼는 게 아무 의미가 없게 됨\n",
    "- 매개변수의 작은 변화가 주는 파장을 계단 함수가 말살하여 손실 함수의 값에는 아무런 변화가 나타나지 않기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fad8c",
   "metadata": {},
   "source": [
    "- 계단 함수는 한순간만 변화를 일으키지만, 시그모이드 함수의 미분(접선)은 출력(세로축의 값)이 연속적으로 변하고 곡선의 기울기도 연속적으로 변함\n",
    "\n",
    "- 시그모이드 함수의 미분은 어느 장소라도 0이 되지는 않는다\n",
    "- 이는 신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않는 덕분에 신경망이 올바르게 학습할 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
